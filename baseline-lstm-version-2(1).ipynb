{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## US Drought & Meteorological Data Starter Notebook\n",
    "This notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Visualizing the Data\n",
    "In this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the json files for training, validation and testing into the ``files`` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "sns.set_style('white')\n",
    "\n",
    "files = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        if 'train' in filename:\n",
    "            files['train'] = os.path.join(dirname, filename)\n",
    "        if 'valid' in filename:\n",
    "            files['valid'] = os.path.join(dirname, filename)\n",
    "        if 'test' in filename:\n",
    "            files['test'] = os.path.join(dirname, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\n",
    "This could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n",
    "    for k in files.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    return (\n",
    "        np.sin(2*np.pi*date.timetuple().tm_yday/366),\n",
    "        np.cos(2*np.pi*date.timetuple().tm_yday/366)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one of 'train', 'valid' or 'test'\n",
    "def loadXY(\n",
    "    df,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    window_size=180,\n",
    "    target_size=12,\n",
    "    fuse_past=False,\n",
    "    return_fips=False,\n",
    "    encode_season=False,\n",
    "):\n",
    "    soil_df = pd.read_csv('soil_data.csv')\n",
    "    time_data_cols = sorted([c for c in df.columns if c not in ['fips','date','score']])\n",
    "    static_data_cols = sorted([c for c in soil_df.columns if c not in ['soil','lat','lon']])\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=['score'])\n",
    "    X_static = np.empty((len(df)//window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty((len(df)//window_size, window_size, len(time_data_cols)+add_dim))\n",
    "    y_past = np.empty((len(df)//window_size, window_size))\n",
    "    y_target = np.empty((len(df)//window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0)==fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df['score'].values\n",
    "        X_s = soil_df[soil_df['fips']==fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y)-(window_size+target_size*7), window_size):\n",
    "            X_fips_date.append((fips,fips_df.index[i:i+window_size][-1]))\n",
    "            X_time[count,:,:len(time_data_cols)] = X[i:i+window_size]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i:i+window_size])\n",
    "            else:\n",
    "                X_time[count,:,len(time_data_cols)] = interpolate_nans(y[i:i+window_size])\n",
    "            if encode_season:\n",
    "                enc_dates = [date_encode(d) for f, d in fips_df.index[i:i+window_size].values]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count,:,len(time_data_cols)+(add_dim-2)] = d_sin\n",
    "                X_time[count,:,len(time_data_cols)+(add_dim-2)+1] = d_cos\n",
    "            temp_y = y[i+window_size:i+window_size+target_size*7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(\n",
    "                X_time[:,:,index].reshape(-1, 1)\n",
    "            )\n",
    "        X_time[:,:,index] = scaler_dict[index].transform(X_time[:,:,index].reshape(-1, 1)).reshape(-1, X_time.shape[-2])\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:,index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:,index] = scaler_dict_static[index].transform(X_static[:,index].reshape(-1, 1)).reshape(1, -1)\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(\n",
    "                y_past.reshape(-1, 1)\n",
    "            )\n",
    "        y_past[:,:] = scaler_dict_past[index].transform(y_past.reshape(-1, 1)).reshape(-1, y_past.shape[-1])\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb351fcf1d469d9838c86358c4f405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_static_train, X_time_train, y_target_train = loadXY(dfs['train'], fuse_past=True, encode_season=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c81bc3084aa48d4a421614cd4c80fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_static_valid, X_time_valid, y_target_valid, valid_fips = loadXY(dfs['valid'], random_state=None, fuse_past=True, return_fips=True, encode_season=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57b2cc1aff24bdcba686249f72aefdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd06034a92f42239fa0b133ae221e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_static_train, X_time_train = normalize(X_static_train, X_time_train, fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063502ad1f1548219d631551916ae534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66010ff9e85949679aaa13c20390e5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_static_valid, X_time_valid = normalize(X_static_valid, X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 128\n",
    "lr = 7e-5\n",
    "output_size = 1\n",
    "hidden_dim = 512\n",
    "dropout = 0.1\n",
    "n_layers = 2\n",
    "epochs = 10\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(X_time_train), torch.tensor(y_target_train[:,0]))\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "valid_data = TensorDataset(torch.tensor(X_time_valid), torch.tensor(y_target_valid[:,0]))\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DroughtNetLSTM(nn.Module):\n",
    "    def __init__(self, output_size, num_input_features, hidden_dim, n_layers, drop_prob=0.2, add_static=False):\n",
    "        super(DroughtNetLSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(num_input_features, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, static=None):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.cuda().to(dtype=torch.float32)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DroughtNetLSTM(\n",
       "  (lstm): LSTM(21, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('using GPU')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('using CPU')\n",
    "\n",
    "\n",
    "model = DroughtNetLSTM(output_size, X_time_train.shape[-1], hidden_dim, n_layers, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470c4fbbbe2d4791adf70ef133bfab3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8793061971664429, 'epoch': 0.5, 'step': 401, 'lr': 7.3052046939180236e-06, 'validation_loss': 0.557697421974606, 'macro_f1': 0.230593548536648, 'micro_f1': 0.7230902777777778, 'None_f1': 0.8698350481789972, 'D0_f1': 0.5137262430408908, 'D1_f1': 0.0, 'D2_f1': 0.0, 'D3_f1': 0.0, 'D4_f1': 0.0}\n",
      "Validation loss decreased (inf --> 0.557697).  Saving model ...\n",
      "{'loss': 0.06499575823545456, 'epoch': 1.0, 'step': 802, 'lr': 1.9612671791499418e-05, 'validation_loss': 0.10352242758704557, 'macro_f1': 0.5870767269519013, 'micro_f1': 0.8684895833333334, 'None_f1': 0.9469095438268722, 'D0_f1': 0.7129551227773073, 'D1_f1': 0.6707734428473648, 'D2_f1': 0.5752808988764045, 'D3_f1': 0.6165413533834586, 'D4_f1': 0.0}\n",
      "Validation loss decreased (0.557697 --> 0.103522).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b64222657e34c778882e279e212c14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.08812756836414337, 'epoch': 1.5, 'step': 1203, 'lr': 3.642194542737123e-05, 'validation_loss': 0.0818952369865858, 'macro_f1': 0.720884762279371, 'micro_f1': 0.8619791666666666, 'None_f1': 0.9419446192573946, 'D0_f1': 0.7008071249652101, 'D1_f1': 0.6821705426356589, 'D2_f1': 0.5688073394495413, 'D3_f1': 0.631578947368421, 'D4_f1': 0.8}\n",
      "Validation loss decreased (0.103522 --> 0.081895).  Saving model ...\n",
      "{'loss': 0.034090444445610046, 'epoch': 2.0, 'step': 1604, 'lr': 5.32253340250048e-05, 'validation_loss': 0.06694786864358725, 'macro_f1': 0.6083025623710278, 'micro_f1': 0.8509114583333334, 'None_f1': 0.9370994540707335, 'D0_f1': 0.6832541235672351, 'D1_f1': 0.6753894080996885, 'D2_f1': 0.5286343612334802, 'D3_f1': 0.4776119402985075, 'D4_f1': 0.34782608695652173}\n",
      "Validation loss decreased (0.081895 --> 0.066948).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b609d5f877924566a3b39b4c6edb1cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.07571909576654434, 'epoch': 2.5, 'step': 2005, 'lr': 6.551672418060321e-05, 'validation_loss': 0.06715978250657725, 'macro_f1': 0.6563620374877978, 'micro_f1': 0.8508029513888888, 'None_f1': 0.9358263349706861, 'D0_f1': 0.6796875000000001, 'D1_f1': 0.6793846153846154, 'D2_f1': 0.5450549450549451, 'D3_f1': 0.5648854961832062, 'D4_f1': 0.5333333333333333}\n",
      "{'loss': 0.03494518622756004, 'epoch': 3.0, 'step': 2406, 'lr': 6.999999451986535e-05, 'validation_loss': 0.06709591403422463, 'macro_f1': 0.6008090423370928, 'micro_f1': 0.8560112847222222, 'None_f1': 0.9416213661073033, 'D0_f1': 0.6968581941692613, 'D1_f1': 0.672544080604534, 'D2_f1': 0.4934497816593887, 'D3_f1': 0.45255474452554745, 'D4_f1': 0.34782608695652173}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1830180347044e10a8986382d1bcd5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.02932736463844776, 'epoch': 4.0, 'step': 3208, 'lr': 6.652542129575786e-05, 'validation_loss': 0.06446024133280541, 'macro_f1': 0.5973752880593434, 'micro_f1': 0.8549262152777778, 'None_f1': 0.9434703986162434, 'D0_f1': 0.6925287356321839, 'D1_f1': 0.6575171553337492, 'D2_f1': 0.47639484978540775, 'D3_f1': 0.4507042253521127, 'D4_f1': 0.3636363636363636}\n",
      "Validation loss decreased (0.065140 --> 0.064460).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dc4b69f11d4673b125219a3e6a77bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.07341979444026947, 'epoch': 4.5, 'step': 3609, 'lr': 6.235191653044496e-05, 'validation_loss': 0.0638828135561198, 'macro_f1': 0.654791209019074, 'micro_f1': 0.8531901041666666, 'None_f1': 0.9396068524512513, 'D0_f1': 0.6827332010206975, 'D1_f1': 0.6670769230769231, 'D2_f1': 0.5324675324675324, 'D3_f1': 0.5735294117647058, 'D4_f1': 0.5333333333333333}\n",
      "Validation loss decreased (0.064460 --> 0.063883).  Saving model ...\n",
      "{'loss': 0.027604930102825165, 'epoch': 5.0, 'step': 4010, 'lr': 5.680687947882316e-05, 'validation_loss': 0.06433719060603632, 'macro_f1': 0.6186951743337877, 'micro_f1': 0.8564453125, 'None_f1': 0.9428706326723325, 'D0_f1': 0.6937142857142857, 'D1_f1': 0.6633354153653965, 'D2_f1': 0.5085470085470085, 'D3_f1': 0.5037037037037037, 'D4_f1': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20c6991e0114e3f8eaa8547d1370865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6/10:   0%|          | 0/802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.07400666177272797, 'epoch': 5.5, 'step': 4411, 'lr': 5.016836145271907e-05, 'validation_loss': 0.06517152549349703, 'macro_f1': 0.6591314639464531, 'micro_f1': 0.8562282986111112, 'None_f1': 0.9406104582380314, 'D0_f1': 0.6878547105561862, 'D1_f1': 0.6765067650676507, 'D2_f1': 0.5450549450549451, 'D3_f1': 0.5714285714285715, 'D4_f1': 0.5333333333333333}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "counter = 0\n",
    "valid_loss_min = np.Inf\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for k, (inputs, labels) in tqdm(enumerate(train_loader), desc=f'epoch {i+1}/{epochs}', total=len(train_loader)):\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = loss_function(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            labels = []\n",
    "            preds = []\n",
    "            for inp, lab in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = loss_function(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                for l in lab:\n",
    "                    labels.append(int(l))\n",
    "                for p in out.round():\n",
    "                    if p > 5:\n",
    "                        p = 5\n",
    "                    if p < 0:\n",
    "                        p = 0\n",
    "                    preds.append(int(p))\n",
    "            \n",
    "            # log data\n",
    "            log_dict = {\n",
    "                'loss': float(loss),\n",
    "                'epoch': counter/len(train_loader),\n",
    "                'step': counter,\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            }\n",
    "            log_dict['validation_loss'] = np.mean(val_losses)\n",
    "            log_dict[f'macro_f1'] = f1_score(labels, preds, average='macro')\n",
    "            log_dict[f'micro_f1'] = f1_score(labels, preds, average='micro')\n",
    "            for j, f1 in enumerate(f1_score(labels, preds, average=None)):\n",
    "                log_dict[f'{id2class[j]}_f1'] = f1\n",
    "            print(log_dict)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "                best_log_dict = log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_h = tuple([each.data for each in model.init_hidden(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    out, _ = model(torch.tensor(x), val_h)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248e1dd9008141d190388cc6c7c0ed5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "dict_map = {\n",
    "    'y_pred': [],\n",
    "    'y_pred_rounded': [],\n",
    "    'fips': [],\n",
    "    'date': [],\n",
    "    'y_true': [],\n",
    "}\n",
    "for x, fips_date, y in tqdm(zip(X_time_valid, valid_fips, y_target_valid), total=len(X_time_valid)):\n",
    "    pred = predict(torch.tensor([x])).clone().detach()[0].float()\n",
    "    if fips_date[1] not in dict_map['fips']:\n",
    "        dict_map['y_pred'].append(float(pred))\n",
    "        dict_map['y_pred_rounded'].append(int(pred.round()))\n",
    "        dict_map['fips'].append(fips_date[1][0])\n",
    "        dict_map['date'].append(fips_date[1][1])\n",
    "        dict_map['y_true'].append(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('drougths_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10358689, 0.368     , 0.09124933, 0.13235294, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df['y_true'].apply(round), df['y_pred_rounded'], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df[df['date']==dates[0]]\n",
    "df1 = df[df['date']==dates[1]]\n",
    "df2 = df[df['date']==dates[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016426703215121925"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df0['y_true'].apply(round), df0['y_pred_rounded'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23451370512081243"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df1['y_true'].apply(round), df1['y_pred_rounded'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04124057804023237"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df2['y_true'].apply(round), df2['y_pred_rounded'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9268948446595375"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df2['y_true'], df2['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6846835148191973"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df1['y_true'], df1['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2092584471222056"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df0['y_true'], df0['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
